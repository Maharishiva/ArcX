{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARC-AGI PPO Training on Colab Pro+ (A100)\n",
    "\n",
    "This notebook trains a PerceiverActorCritic model (~4.8M params) on the ARC-AGI-2 dataset using PPO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt-get update\n",
    "%pip install -U pip wandb\n",
    "%pip install \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html wandb\n",
    "%pip install flax optax orbax-checkpoint tensorstore imageio einops matplotlib pillow wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify GPU is Available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Device type: {jax.devices()[0].device_kind}\")\n",
    "print(f\"Device platform: {jax.devices()[0].platform}\")\n",
    "\n",
    "# Quick GPU test\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (1000, 1000))\n",
    "_ = jnp.dot(x, x).block_until_ready()\n",
    "print(\"âœ… GPU is working!\")\n",
    "\n",
    "# Check GPU memory\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone Repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Maharishiva/ArcX.git\n",
    "%cd ArcX\n",
    "!git status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d59d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "\n",
    "repo_root = pathlib.Path.cwd()\n",
    "current_path = os.environ.get('PYTHONPATH')\n",
    "paths = [str(repo_root)] + ([current_path] if current_path else [])\n",
    "os.environ['PYTHONPATH'] = ':'.join(paths)\n",
    "os.environ.setdefault('FLAX_USE_ORBAX_CHECKPOINTING', '0')\n",
    "print('PYTHONPATH set to:', os.environ['PYTHONPATH'])\n",
    "print('FLAX_USE_ORBAX_CHECKPOINTING =', os.environ['FLAX_USE_ORBAX_CHECKPOINTING'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Simple Datasets (Optional)\n",
    "\n",
    "Fetch the latest ARC-AGI-2 data, copy it into this repo, and generate intentionally easy `training_simple` / `val_simple` splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "cd /content/ArcX\n",
    "mkdir -p data/training data/evaluation\n",
    "if [ ! -d /content/arc-agi-2 ]; then\n",
    "  git clone https://github.com/arcprize/ARC-AGI-2.git /content/arc-agi-2\n",
    "else\n",
    "  echo 'ARC-AGI-2 repository already exists; pulling latest changes.'\n",
    "  git -C /content/arc-agi-2 pull --ff-only\n",
    "fi\n",
    "cp -f /content/arc-agi-2/data/training/*.json data/training/\n",
    "cp -f /content/arc-agi-2/data/evaluation/*.json data/evaluation/\n",
    "echo 'training count:'\n",
    "ls -1 data/training | wc -l\n",
    "echo 'evaluation count:'\n",
    "ls -1 data/evaluation | wc -l\n",
    "python scripts/make_simple_datasets.py\n",
    "ls data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (Optional) Enable Weights & Biases Logging\n",
    "\n",
    "Authenticate with wandb so training runs can report metrics.\n",
    "Add `--wandb-mode online --wandb-project <your_project>` to any training command when you want logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Smoke Test (Optional, ~2 minutes)\n",
    "\n",
    "Run this to verify everything is working before starting a long training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/ppo_train.py \\\n",
    "    --preset debug \\\n",
    "    --device cuda \\\n",
    "    --total-updates 5 \\\n",
    "    --num-envs 4 \\\n",
    "    --rollout-length 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Training Run\n",
    "\n",
    "Choose one of the options below based on how long you want to train:\n",
    "\n",
    "- **Quick test (~30 min):** 100 updates\n",
    "- **Medium run (~2-3 hours):** 500 updates  \n",
    "- **Full training (~6-8 hours):** 1000+ updates\n",
    "\n",
    "Uncomment the option you want to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-run PPO training (simple data split)\n",
    "\n",
    "Uses `data/training_simple` to accelerate convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Long-run PPO training (simple data split)\n",
    "!python scripts/ppo_train.py   --device cuda   --num-envs 16   --rollout-length 64   --num-minibatches 4   --num-epochs 2   --eval-envs 8   --eval-horizon 512   --total-updates 10000   --eval-interval 25   --checkpoint-interval 100   --checkpoint-dir ${drive_dir:-checkpoints/ppo_a100}   --data-dir data/training_simple   --wandb-mode online   --wandb-project arc-agi-ppo   --wandb-run-name colab_long_run_simple\n",
    "\n",
    "# Option A: Quick test run (~30 minutes)\n",
    "# Add --wandb-mode online --wandb-project <your_project> to enable wandb logging\n",
    "!python scripts/ppo_train.py     --preset a100     --device cuda     --total-updates 100     --eval-interval 20     --checkpoint-interval 25     --checkpoint-dir ${drive_dir:-checkpoints/ppo_a100}     --data-dir data/training_simple\n",
    "\n",
    "# Option C: Full training run (~6-8 hours)\n",
    "# !python scripts/ppo_train.py #     --preset a100 #     --device cuda #     --total-updates 1000 #     --eval-interval 50 #     --checkpoint-interval 50 #     --checkpoint-dir ${drive_dir:-checkpoints/ppo_a100}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Quick test run (~30 minutes)\n",
    "# Add --wandb-mode online --wandb-project <your_project> to enable wandb logging\n",
    "!PYTHONPATH=. python scripts/ppo_train.py \\\n",
    "    --preset a100 \\\n",
    "    --device cuda \\\n",
    "    --total-updates 100 \\\n",
    "    --eval-interval 20 \\\n",
    "    --checkpoint-interval 25 \\\n",
    "    --data-dir data/training_simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Medium run (~2-3 hours) - RECOMMENDED FOR FIRST RUN\n",
    "# !PYTHONPATH=. python scripts/ppo_train.py \\\n",
    "#     --preset a100 \\\n",
    "#     --device cuda \\\n",
    "#     --total-updates 500 \\\n",
    "#     --eval-interval 25 \\\n",
    "#     --checkpoint-interval 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option C: Full training run (~6-8 hours)\n",
    "# !PYTHONPATH=. python scripts/ppo_train.py \\\n",
    "#     --preset a100 \\\n",
    "#     --device cuda \\\n",
    "#     --total-updates 1000 \\\n",
    "#     --eval-interval 50 \\\n",
    "#     --checkpoint-interval 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. List Available Checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_dir = Path(\"checkpoints/ppo_a100\")\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoints = sorted([d for d in checkpoint_dir.iterdir() if d.is_dir()])\n",
    "    print(f\"Found {len(checkpoints)} checkpoints:\")\n",
    "    for ckpt in checkpoints:\n",
    "        size_mb = sum(f.stat().st_size for f in ckpt.rglob('*') if f.is_file()) / 1024 / 1024\n",
    "        print(f\"  - {ckpt.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"No checkpoints found yet. Run training first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model & Generate GIFs\n",
    "\n",
    "This will create GIF visualizations of the trained model solving ARC puzzles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest checkpoint\n",
    "checkpoint_dir = Path(\"checkpoints/ppo_a100\")\n",
    "checkpoints = sorted([d for d in checkpoint_dir.iterdir() if d.is_dir()])\n",
    "latest_checkpoint = checkpoints[-1] if checkpoints else None\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(f\"Evaluating checkpoint: {latest_checkpoint}\")\n",
    "    !python scripts/ppo_eval_viz.py \\\n",
    "        --checkpoint {latest_checkpoint} \\\n",
    "        --device cuda \\\n",
    "        --num-episodes 10 \\\n",
    "        --rollout-horizon 100 \\\n",
    "        --output-dir artifacts/eval_viz\n",
    "else:\n",
    "    print(\"No checkpoints found. Run training first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Generated GIFs\n",
    "\n",
    "View the generated GIFs directly in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage, display\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"artifacts/eval_viz\")\n",
    "if output_dir.exists():\n",
    "    gifs = sorted(output_dir.glob(\"*.gif\"))\n",
    "    print(f\"Found {len(gifs)} GIFs:\\n\")\n",
    "    \n",
    "    for gif in gifs[:5]:  # Show first 5\n",
    "        print(f\"Episode: {gif.name}\")\n",
    "        display(IPImage(filename=str(gif)))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "else:\n",
    "    print(\"No GIFs found. Run evaluation first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download Results\n",
    "\n",
    "Download GIFs and checkpoints to your local machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download GIFs\n",
    "!zip -r eval_results.zip artifacts/eval_viz/ 2>/dev/null || echo \"Creating zip...\"\n",
    "\n",
    "from google.colab import files\n",
    "if Path(\"eval_results.zip\").exists():\n",
    "    print(\"Downloading GIFs...\")\n",
    "    files.download('eval_results.zip')\n",
    "else:\n",
    "    print(\"No results to download yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced: Mount Google Drive (Optional)\n",
    "\n",
    "Run the next cell to mount Google Drive, create a timestamped directory, and point training checkpoints there automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "run_name = f\"arcx_ppo_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "drive_dir = f\"/content/drive/MyDrive/arcx_ppo/{run_name}\"\n",
    "os.makedirs(drive_dir, exist_ok=True)\n",
    "print('Saving checkpoints to:', drive_dir)\n",
    "\n",
    "# Export path for subsequent shell commands\n",
    "os.environ['drive_dir'] = drive_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
